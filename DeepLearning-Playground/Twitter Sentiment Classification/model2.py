import sysimport warningsif not sys.warnoptions:    warnings.simplefilter("ignore")from sklearn.feature_extraction.text import CountVectorizerfrom keras.preprocessing.text import Tokenizerfrom keras.preprocessing.sequence import pad_sequencesfrom keras.models import Sequentialfrom keras.layers import Dense, Embedding, LSTM , Bidirectional , GRUfrom keras.layers import Flatten, Dropout , Activation ,BatchNormalizationfrom keras.layers import Conv1D, GlobalMaxPooling1D , MaxPooling1D  ,Merge, Averagefrom sklearn.model_selection import train_test_splitfrom keras.utils.np_utils import to_categoricalfrom keras.preprocessing.text import BatchNormalizationimport reimport pandas as pdimport numpy as npfrom textblob import TextBlobfrom nltk.corpus import stopwordsnp.random.seed(7)MAX_NUM_WORDS = 10000MAX_SEQUENCE_LENGTH=1000EMBED_DIM = 128LSTM_OUT = 196BATCH_SIZE =64train=pd.read_csv('train_E6oV3lV.csv')test=pd.read_csv('test_tweets_anuFYb8.csv')target=train.labeltrain=train.drop('label',1)data=train.append(test)# pre-processingprint("Text Preprocessing ====>")data['tweet']=data['tweet'].apply(lambda x: x.lower())data['tweet']=data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))data['tweet']=data['tweet'].apply(lambda x: x.replace('user',''))stop = stopwords.words('english')data['tweet'] = data['tweet'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))# prepare tokenizert=Tokenizer(num_words=MAX_NUM_WORDS)t.fit_on_texts(data['tweet'])vocab_size=len(t.word_index)+1print('VOCAB SIZE',vocab_size)# integer encode the documentsencoded_docs = t.texts_to_sequences(data['tweet'])padded_docs = pad_sequences(encoded_docs,  padding='post')#padded_docs=np.hstack((padded_docs,sentiment))# create a weight matrix for words in training docsembedding_matrix = np.ones((vocab_size, EMBED_DIM))*0.01train_Seq=padded_docs[:len(train)]test_Seq=padded_docs[len(train):]print("GRU Model Training ---")model1 = Sequential()model1.add(Embedding(vocab_size, EMBED_DIM, weights=[embedding_matrix] , input_length = padded_docs.shape[1], trainable=True))model1.add(BatchNormalization())model1.add(Bidirectional(GRU(LSTM_OUT,return_sequences=True)))model1.add(Dropout(0.2))model1.add(BatchNormalization())model1.add(Flatten())model1.add(Dense(LSTM_OUT//2,activation='relu'))  model2=Sequential()model2.add(Embedding(vocab_size, EMBED_DIM, weights=[embedding_matrix] , input_length = padded_docs.shape[1], trainable=True))model2.add(BatchNormalization())model2.add(Conv1D(32,3,activation='relu'))model2.add(MaxPooling1D(3))model2.add(BatchNormalization())model2.add(Conv1D(64,3,activation='relu'))model2.add(MaxPooling1D(3))model2.add(BatchNormalization())model2.add(Conv1D(128,3,activation='relu'))model2.add(GlobalMaxPooling1D())model2.add(BatchNormalization())model2.add(Dense(LSTM_OUT//2,activation='relu'))  model1.add(Dense(2,activation='softmax'))model1.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])print(model1.summary())model2.add(Dense(2,activation='softmax'))model2.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])print(model2.summary())Y = pd.get_dummies(target).valuesX_train, X_test, Y_train, Y_test = train_test_split(train_Seq,Y, test_size = 0.3, random_state = 42)model1.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=3, batch_size=BATCH_SIZE)model2.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=3, batch_size=BATCH_SIZE)prediction1=model1.predict_proba(test_Seq)prediction2=model2.predict_proba(test_Seq)prediction=prediction2*0.5+prediction1*0.5prediction=np.asarray(prediction)def map(val):  val=list(val)  return val.index(max(list(val)))  if val>0.5:    return 1  else:    return 0prediction=[map(x) for x in prediction]submission=pd.DataFrame()submission['id']=test['id']submission['label']=predictionsubmission.to_csv('GRUsub.csv',index=False)