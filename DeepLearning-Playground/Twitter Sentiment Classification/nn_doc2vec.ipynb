{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing ====>\n",
      "Train set has total 22373 entries with 92.93 % positive, 7.07% negative\n",
      "Validation set has total 4794 entries with 92.89 % positive, 7.11% negative\n",
      "Test set has total 4795 entries with 93.33 % positive, 6.67% negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "\n",
    "\n",
    "train=pd.read_csv('train_E6oV3lV.csv')\n",
    "test=pd.read_csv('test_tweets_anuFYb8.csv')\n",
    "\n",
    "target=train.label\n",
    "train=train.drop('label',1)\n",
    "data=train.append(test)\n",
    "tweet =data.tweet\n",
    "\n",
    "\n",
    "# pre-processing\n",
    "print(\"Text Preprocessing ====>\")\n",
    "data['tweet']=data['tweet'].apply(lambda x: x.lower())\n",
    "data['tweet']=data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['tweet']=data['tweet'].apply(lambda x: x.replace('user',''))\n",
    "stop = stopwords.words('english')\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "x = data['tweet'][:len(train)]\n",
    "y = target\n",
    "\n",
    "x_train, x_validation_and_test , y_train , y_validation_and_test = train_test_split(x, y,test_size=.3,random_state= SEED)\n",
    "\n",
    "x_validation , x_test , y_validation , y_test = train_test_split(x_validation_and_test,y_validation_and_test,test_size=0.5,random_state=SEED)\n",
    "\n",
    "\n",
    "print(\"Train set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_train),(len(x_train[y_train==0])/(len(x_train)*1.))*100,\n",
    "                                                                                          (len(x_train[y_train == 1]) / (len(x_train) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Validation set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_validation),(len(x_validation[y_validation==0])/(len(x_validation)*1.))*100,\n",
    "                                                                                          (len(x_validation[y_validation == 1]) / (len(x_validation) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_test),(len(x_test[y_test==0])/(len(x_test)*1.))*100,\n",
    "                                                                                          (len(x_test[y_test == 1]) / (len(x_test) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def labelize_tweets_ug(tweets,label):\n",
    "\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i , t in zip(tweets.index,tweets):\n",
    "        result.append(TaggedDocument(t.split(),[prefix + '_%s' % i ]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x,'all')\n",
    "\n",
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n= 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "\n",
    "    return vecs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 2402307.08it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2429473.44it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2554076.06it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2392531.85it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2507825.96it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2658358.18it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2430222.15it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2545153.87it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2334697.74it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2747941.88it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2341017.10it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3091109.88it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2939553.66it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2236057.32it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2875554.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2591801.57it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2546314.09it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2820024.92it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2457666.68it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2601710.65it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2471212.66it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2999470.72it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2346877.64it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2561640.73it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2520272.68it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2523831.25it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2785627.94it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3015800.06it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3025259.96it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2935434.20it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2508623.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.942636629119733\n"
     ]
    }
   ],
   "source": [
    "# Distributed Bag of Words\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model_ug_dbow = Doc2Vec(dm=0,vector_size=100,negative=5,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v),epochs=1)\n",
    "    model_ug_dbow.alpha -=0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha\n",
    "\n",
    "train_vecs_dbow = get_vectors(model_ug_dbow, x_train, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow, y_train)\n",
    "print('score',clf.score(validation_vecs_dbow,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases , Phraser\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "tokenized_train = [t.split() for t in all_x]\n",
    "phrases = Phrases(tokenized_train)\n",
    "bigram = Phraser(phrases)\n",
    "tg_phrases = Phrases(bigram[tokenized_train])\n",
    "trigram = Phraser(tg_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_tg(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(trigram[bigram[t.split()]], [prefix + '_%s' % i]))\n",
    "    return result\n",
    "  \n",
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v_tg = labelize_tweets_tg(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 1986461.55it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2509375.07it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2712468.78it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2579434.01it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3007883.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3095606.72it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2678702.48it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2911210.76it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2738567.26it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3008018.14it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2824005.07it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2724375.48it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2748223.54it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2702516.77it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2957517.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2515873.97it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2602316.69it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2529211.84it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2553103.23it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1995509.74it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2680255.60it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2552276.91it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2687240.05it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2682722.87it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3052469.25it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2547814.29it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2623502.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2808622.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2690152.00it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2636919.38it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2682293.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9336670838548186\n"
     ]
    }
   ],
   "source": [
    "# Distributed Memory Mean\n",
    "\n",
    "model_tg_dmm = Doc2Vec(dm=1,dm_mean=1,vector_size=100,negative=5,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_tg_dmm.build_vocab([x for x in tqdm(all_x_w2v_tg)])\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_tg_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v_tg)]), total_examples=len(all_x_w2v_tg),epochs=1)\n",
    "    model_tg_dmm.alpha -=0.002\n",
    "    model_tg_dmm.min_alpha = model_tg_dmm.alpha\n",
    "\n",
    "train_vecs_dmm = get_vectors(model_tg_dmm, x_train, 100)\n",
    "validation_vecs_dmm = get_vectors(model_tg_dmm, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, y_train)\n",
    "print('score',clf.score(validation_vecs_dmm,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9451124122826622\n",
      "0.9451397580308719\n"
     ]
    }
   ],
   "source": [
    "model_ug_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_tg_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "train_vecs_ugdbow_tgdmm = get_concat_vectors(model_ug_dbow,model_tg_dmm, x_train, 200)\n",
    "validation_vecs_ugdbow_tgdmm = get_concat_vectors(model_ug_dbow,model_tg_dmm, x_validation, 200)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_ugdbow_tgdmm, y_train)\n",
    "\n",
    "print(clf.score(train_vecs_ugdbow_tgdmm, y_train))\n",
    "print(clf.score(validation_vecs_ugdbow_tgdmm, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 7\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22373 samples, validate on 4794 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.1639 - acc: 0.9418 - val_loss: 0.1388 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.94973, saving model to d2v_09_best_weights.01-0.9497.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.1211 - acc: 0.9565 - val_loss: 0.1347 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.94973 to 0.94973, saving model to d2v_09_best_weights.02-0.9497.hdf5\n",
      "Epoch 3/100\n",
      " - 2s - loss: 0.0933 - acc: 0.9656 - val_loss: 0.1628 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.94973 to 0.95140, saving model to d2v_09_best_weights.03-0.9514.hdf5\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.0651 - acc: 0.9759 - val_loss: 0.1553 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.95140\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.0423 - acc: 0.9844 - val_loss: 0.2123 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.95140\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.0316 - acc: 0.9886 - val_loss: 0.2411 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.95140\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0233 - acc: 0.9916 - val_loss: 0.2823 - val_acc: 0.9481\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.95140\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.0206 - acc: 0.9927 - val_loss: 0.2503 - val_acc: 0.9443\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.95140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4650cf28>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "filepath=\"d2v_09_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=5, mode='max') \n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "np.random.seed(seed)\n",
    "model_d2v_09_es = Sequential()\n",
    "model_d2v_09_es.add(Dense(256, activation='relu', input_dim=200))\n",
    "model_d2v_09_es.add(Dense(256, activation='relu'))\n",
    "model_d2v_09_es.add(Dense(256, activation='relu'))\n",
    "model_d2v_09_es.add(Dense(1, activation='sigmoid'))\n",
    "model_d2v_09_es.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_d2v_09_es.fit(train_vecs_ugdbow_tgdmm, y_train,\n",
    "                    validation_data=(validation_vecs_ugdbow_tgdmm, y_validation), \n",
    "                    epochs=100, batch_size=32, verbose=2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4794/4794 [==============================] - 0s 39us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16283375229632102, 0.9513975801595211]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model('d2v_09_best_weights.03-0.9514.hdf5')\n",
    "loaded_model.evaluate(x=validation_vecs_ugdbow_tgdmm, y=y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = loaded_model.predict_classes(get_concat_vectors(model_ug_dbow,model_tg_dmm, data[len(train):], 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['label']=pred\n",
    "submission.to_csv('submission_twitter_200d.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
