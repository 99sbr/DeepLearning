{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stop\n",
    "words\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('./data/train_E6oV3lV.csv')\n",
    "test=pd.read_csv('./data/test_tweets_anuFYb8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "train=train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi = train.append(test, ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")\n",
    "\n",
    "# remove special characters, numbers, punctuations\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace('#',' Hashtag ')\n",
    "\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [lmtzr.lemmatize(i) for i in x]) #lemmatize \n",
    "tokenized_tweet.head()\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "combi['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=combi[:len(train)]\n",
    "df_test=combi[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    29720\n",
       "1.0     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>31831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user it's off to watch the football game bet...</td>\n",
       "      <td>off watch the footbal game between hashtag rom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i was watching justin's new clip and thinking ...</td>\n",
       "      <td>wa watch justin new clip and think abt day fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user just opened packs for 2mill coins, best ...</td>\n",
       "      <td>just open pack for mill coin best player you a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>17619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@user new video!  super mario run is sexist!  ...</td>\n",
       "      <td>new video super mario run sexist hashtag chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#duschszene #fear #origins #pib #moore   #temp...</td>\n",
       "      <td>hashtag duschszen hashtag fear hashtag origin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                              tweet  \\\n",
       "0  31831    0.0   @user it's off to watch the football game bet...   \n",
       "1   6163    0.0  i was watching justin's new clip and thinking ...   \n",
       "2  15562    0.0  @user just opened packs for 2mill coins, best ...   \n",
       "3  17619    1.0  @user new video!  super mario run is sexist!  ...   \n",
       "4  17074    0.0  #duschszene #fear #origins #pib #moore   #temp...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  off watch the footbal game between hashtag rom...  \n",
       "1  wa watch justin new clip and think abt day fam...  \n",
       "2  just open pack for mill coin best player you a...  \n",
       "3  new video super mario run sexist hashtag chris...  \n",
       "4  hashtag duschszen hashtag fear hashtag origin ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-13 16:54:30,384 loading file /Users/subir/.flair/models/imdb-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings , BertEmbeddings ,CharacterEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train[['label', 'tidy_tweet']].rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
    "train['label'] = '__label__' + train['label'].astype(str)\n",
    "train.iloc[0:int(len(train)*0.8)].to_csv('train.csv', sep='\\t', index = False, header = False)\n",
    "train.iloc[int(len(train)*0.8):int(len(train)*0.9)].to_csv('test.csv', sep='\\t', index = False, header = False)\n",
    "train.iloc[int(len(train)*0.9):].to_csv('dev.csv', sep='\\t', index = False, header = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-13 16:55:44,876 Reading data from .\n",
      "2020-01-13 16:55:44,878 Train: train.csv\n",
      "2020-01-13 16:55:44,879 Dev: dev.csv\n",
      "2020-01-13 16:55:44,881 Test: test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/pythonenv/default/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/subir/pythonenv/default/lib/python3.7/site-packages/flair/data_fetcher.py:447: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/Users/subir/pythonenv/default/lib/python3.7/site-packages/flair/data_fetcher.py:454: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/Users/subir/pythonenv/default/lib/python3.7/site-packages/flair/data_fetcher.py:463: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-13 16:55:53,308 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/pythonenv/default/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "100%|██████████| 25561/25561 [00:00<00:00, 197428.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-13 16:55:53,446 [b'0.0', b'1.0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('.'),train_file='train.csv',test_file='test.csv',dev_file='dev.csv')\n",
    "word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('en-forward'), FlairEmbeddings('en-backward'),CharacterEmbeddings()]\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=128, reproject_words=True, reproject_words_dimension=64)\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-13 16:57:51,783 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-13 16:57:51,785 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentLSTMEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_3): CharacterEmbeddings(\n",
      "        (char_embedding): Embedding(275, 25)\n",
      "        (char_rnn): LSTM(25, 25, bidirectional=True)\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=4246, out_features=64, bias=True)\n",
      "    (rnn): GRU(64, 128)\n",
      "    (dropout): Dropout(p=0.5)\n",
      "  )\n",
      "  (decoder): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2020-01-13 16:57:51,787 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-13 16:57:51,791 Corpus: \"Corpus: 25561 train + 3196 dev + 3195 test sentences\"\n",
      "2020-01-13 16:57:51,793 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-13 16:57:51,794 Parameters:\n",
      "2020-01-13 16:57:51,797  - learning_rate: \"0.1\"\n",
      "2020-01-13 16:57:51,800  - mini_batch_size: \"2\"\n",
      "2020-01-13 16:57:51,802  - patience: \"3\"\n",
      "2020-01-13 16:57:51,803  - anneal_factor: \"0.5\"\n",
      "2020-01-13 16:57:51,804  - max_epochs: \"5\"\n",
      "2020-01-13 16:57:51,809  - shuffle: \"True\"\n",
      "2020-01-13 16:57:51,811  - train_with_dev: \"False\"\n",
      "2020-01-13 16:57:51,812 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-13 16:57:51,813 Model training base path: \"falir_model\"\n",
      "2020-01-13 16:57:51,815 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-13 16:57:51,816 Device: cpu\n",
      "2020-01-13 16:57:51,819 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-13 16:57:51,820 Embeddings storage mode: cpu\n",
      "2020-01-13 16:57:51,824 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-13 16:57:53,373 epoch 1 - iter 0/12781 - loss 0.46421811 - samples/sec: 1671.43\n",
      "2020-01-13 17:31:35,020 epoch 1 - iter 1278/12781 - loss 0.24011991 - samples/sec: 1.26\n",
      "2020-01-13 18:09:55,749 epoch 1 - iter 2556/12781 - loss 0.24902700 - samples/sec: 1.11\n",
      "2020-01-13 18:53:07,545 epoch 1 - iter 3834/12781 - loss 0.24840542 - samples/sec: 0.99\n",
      "2020-01-13 19:37:32,693 epoch 1 - iter 5112/12781 - loss 0.24189032 - samples/sec: 0.96\n",
      "2020-01-13 20:14:50,284 epoch 1 - iter 6390/12781 - loss 0.24015590 - samples/sec: 1.14\n",
      "2020-01-13 20:50:19,842 epoch 1 - iter 7668/12781 - loss 0.23965433 - samples/sec: 1.20\n",
      "2020-01-13 21:24:42,010 epoch 1 - iter 8946/12781 - loss 0.23734723 - samples/sec: 1.24\n",
      "2020-01-13 22:01:55,360 epoch 1 - iter 10224/12781 - loss 0.23770423 - samples/sec: 1.14\n",
      "2020-01-13 22:35:06,571 epoch 1 - iter 11502/12781 - loss 0.23806829 - samples/sec: 1.28\n",
      "2020-01-13 23:09:16,168 epoch 1 - iter 12780/12781 - loss 0.23839824 - samples/sec: 1.25\n",
      "2020-01-13 23:09:16,246 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-13 23:09:16,247 EPOCH 1 done: loss 0.2384 - lr 0.1000\n",
      "2020-01-14 00:21:32,352 DEV : loss 0.18039727210998535 - score 0.954\n",
      "2020-01-14 00:21:33,595 BAD EPOCHS (no improvement): 0\n",
      "2020-01-14 00:21:56,704 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-14 00:21:57,643 epoch 2 - iter 0/12781 - loss 0.00364685 - samples/sec: 2967.47\n",
      "2020-01-14 00:28:26,323 epoch 2 - iter 1278/12781 - loss 0.21259927 - samples/sec: 6.60\n",
      "2020-01-14 00:33:43,380 epoch 2 - iter 2556/12781 - loss 0.20034395 - samples/sec: 8.09\n",
      "2020-01-14 00:38:58,919 epoch 2 - iter 3834/12781 - loss 0.21295115 - samples/sec: 8.13\n",
      "2020-01-14 00:44:45,407 epoch 2 - iter 5112/12781 - loss 0.21258116 - samples/sec: 7.40\n",
      "2020-01-14 00:50:13,225 epoch 2 - iter 6390/12781 - loss 0.21139714 - samples/sec: 7.82\n",
      "2020-01-14 00:55:36,725 epoch 2 - iter 7668/12781 - loss 0.21422789 - samples/sec: 7.93\n",
      "2020-01-14 01:01:29,546 epoch 2 - iter 8946/12781 - loss 0.21375573 - samples/sec: 7.27\n",
      "2020-01-14 01:07:06,807 epoch 2 - iter 10224/12781 - loss 0.21338309 - samples/sec: 7.60\n",
      "2020-01-14 01:12:24,062 epoch 2 - iter 11502/12781 - loss 0.21002960 - samples/sec: 8.08\n",
      "2020-01-14 01:17:15,258 epoch 2 - iter 12780/12781 - loss 0.20967088 - samples/sec: 8.81\n",
      "2020-01-14 01:17:15,339 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-14 01:17:15,341 EPOCH 2 done: loss 0.2097 - lr 0.1000\n",
      "2020-01-14 02:27:26,998 DEV : loss 0.16781587898731232 - score 0.9471\n",
      "2020-01-14 02:27:27,388 BAD EPOCHS (no improvement): 1\n",
      "2020-01-14 02:27:58,946 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-14 02:27:59,861 epoch 3 - iter 0/12781 - loss 0.01619720 - samples/sec: 2883.27\n",
      "2020-01-14 02:32:51,648 epoch 3 - iter 1278/12781 - loss 0.19044457 - samples/sec: 8.79\n",
      "2020-01-14 02:37:53,638 epoch 3 - iter 2556/12781 - loss 0.18061815 - samples/sec: 8.50\n",
      "2020-01-14 02:42:38,384 epoch 3 - iter 3834/12781 - loss 0.18263334 - samples/sec: 9.01\n",
      "2020-01-14 02:47:27,864 epoch 3 - iter 5112/12781 - loss 0.18018099 - samples/sec: 8.86\n",
      "2020-01-14 02:52:20,811 epoch 3 - iter 6390/12781 - loss 0.18319493 - samples/sec: 8.75\n",
      "2020-01-14 02:57:10,460 epoch 3 - iter 7668/12781 - loss 0.18625566 - samples/sec: 8.85\n",
      "2020-01-14 03:01:58,899 epoch 3 - iter 8946/12781 - loss 0.18795433 - samples/sec: 8.89\n",
      "2020-01-14 03:06:45,376 epoch 3 - iter 10224/12781 - loss 0.18991279 - samples/sec: 8.95\n",
      "2020-01-14 03:11:27,584 epoch 3 - iter 11502/12781 - loss 0.18990946 - samples/sec: 9.09\n",
      "2020-01-14 03:16:05,644 epoch 3 - iter 12780/12781 - loss 0.18833756 - samples/sec: 9.22\n",
      "2020-01-14 03:16:05,731 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-14 03:16:05,734 EPOCH 3 done: loss 0.1883 - lr 0.1000\n",
      "2020-01-14 04:08:56,147 DEV : loss 0.15293212234973907 - score 0.9559\n",
      "2020-01-14 04:08:56,460 BAD EPOCHS (no improvement): 0\n",
      "2020-01-14 04:09:32,268 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-14 04:09:33,009 epoch 4 - iter 0/12781 - loss 0.00937951 - samples/sec: 3604.75\n",
      "2020-01-14 04:15:22,181 epoch 4 - iter 1278/12781 - loss 0.21573972 - samples/sec: 7.33\n",
      "2020-01-14 04:20:30,302 epoch 4 - iter 2556/12781 - loss 0.19083082 - samples/sec: 8.32\n",
      "2020-01-14 04:25:18,283 epoch 4 - iter 3834/12781 - loss 0.18631712 - samples/sec: 8.89\n",
      "2020-01-14 04:30:06,869 epoch 4 - iter 5112/12781 - loss 0.18098752 - samples/sec: 8.88\n",
      "2020-01-14 04:34:48,267 epoch 4 - iter 6390/12781 - loss 0.18101471 - samples/sec: 9.10\n",
      "2020-01-14 04:39:34,570 epoch 4 - iter 7668/12781 - loss 0.18261925 - samples/sec: 8.95\n",
      "2020-01-14 04:44:19,196 epoch 4 - iter 8946/12781 - loss 0.18253956 - samples/sec: 9.00\n",
      "2020-01-14 04:48:57,895 epoch 4 - iter 10224/12781 - loss 0.18005178 - samples/sec: 9.19\n",
      "2020-01-14 04:53:34,713 epoch 4 - iter 11502/12781 - loss 0.18096615 - samples/sec: 9.25\n",
      "2020-01-14 04:58:05,087 epoch 4 - iter 12780/12781 - loss 0.18296478 - samples/sec: 9.47\n",
      "2020-01-14 04:58:05,164 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-14 04:58:05,167 EPOCH 4 done: loss 0.1830 - lr 0.1000\n",
      "2020-01-14 05:50:31,793 DEV : loss 0.14616960287094116 - score 0.954\n",
      "2020-01-14 05:50:32,266 BAD EPOCHS (no improvement): 1\n",
      "2020-01-14 05:51:02,922 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-14 05:51:04,324 epoch 5 - iter 0/12781 - loss 0.00263643 - samples/sec: 1863.09\n",
      "2020-01-14 05:56:15,020 epoch 5 - iter 1278/12781 - loss 0.15423717 - samples/sec: 8.24\n",
      "2020-01-14 06:01:26,476 epoch 5 - iter 2556/12781 - loss 0.15985416 - samples/sec: 8.23\n",
      "2020-01-14 06:06:16,884 epoch 5 - iter 3834/12781 - loss 0.16379583 - samples/sec: 8.82\n",
      "2020-01-14 06:11:01,195 epoch 5 - iter 5112/12781 - loss 0.17260489 - samples/sec: 9.01\n",
      "2020-01-14 06:15:45,161 epoch 5 - iter 6390/12781 - loss 0.17370627 - samples/sec: 9.02\n",
      "2020-01-14 06:20:29,758 epoch 5 - iter 7668/12781 - loss 0.17592170 - samples/sec: 9.00\n",
      "2020-01-14 06:25:12,577 epoch 5 - iter 8946/12781 - loss 0.17735764 - samples/sec: 9.05\n",
      "2020-01-14 06:29:53,744 epoch 5 - iter 10224/12781 - loss 0.17806942 - samples/sec: 9.11\n",
      "2020-01-14 06:34:36,105 epoch 5 - iter 11502/12781 - loss 0.18129507 - samples/sec: 9.07\n",
      "2020-01-14 06:39:13,122 epoch 5 - iter 12780/12781 - loss 0.18272137 - samples/sec: 9.25\n",
      "2020-01-14 06:39:13,194 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-14 06:39:13,196 EPOCH 5 done: loss 0.1827 - lr 0.1000\n",
      "2020-01-14 07:33:33,359 DEV : loss 0.16041307151317596 - score 0.959\n",
      "2020-01-14 07:33:33,669 BAD EPOCHS (no improvement): 0\n",
      "2020-01-14 07:34:08,711 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-14 07:34:08,714 Testing using best model ...\n",
      "2020-01-14 07:34:08,749 loading file falir_model/best-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/pythonenv/default/lib/python3.7/site-packages/torch/serialization.py:574: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  result = unpickler.load()\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('./falir_model/', max_epochs=5,checkpoint=True,monitor_train=True,mini_batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves('loss.tsv')\n",
    "plotter.plot_weights('weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier.load('./best-model.pt')\n",
    "sentence = Sentence('the love and hate country')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(sentence)\n",
    "sentence.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((str(sentence.labels[0]).split('(')[0]).strip()), float(str(sentence.labels[0]).split('(')[1].split(')')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('./data/test_tweets_anuFYb8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_pred(row):\n",
    "    sentence = Sentence(row['tidy_tweet'])\n",
    "    classifier.predict(sentence)\n",
    "    label= int(float((str(sentence.labels[0]).split('(')[0])))\n",
    "    if label==0:\n",
    "        if float(str(sentence.labels[0]).split('(')[1].split(')')[0])>0.5:\n",
    "            row['label']=0\n",
    "        else:\n",
    "            row['label']=1\n",
    "        \n",
    "    return row\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_test.apply(get_pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.label=pred_df.label.apply(lambda x : int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.drop(['tweet'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('flair_sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
