{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:15:46.419555Z",
     "start_time": "2020-05-15T17:15:31.924181Z"
    }
   },
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "# Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM,Input\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping , CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional,GlobalMaxPool1D, Lambda\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam,Nadam\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "#spacy\n",
    "import wordninja\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tknzr = TweetTokenizer()\n",
    "# import spaCy's language model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:44:29.173037Z",
     "start_time": "2020-05-15T18:44:29.046736Z"
    }
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./data/train_E6oV3lV.csv')\n",
    "test=pd.read_csv('./data/test_tweets_anuFYb8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:49:04.464791Z",
     "start_time": "2020-05-15T18:49:04.447410Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()\n",
    "train=train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:49:05.095825Z",
     "start_time": "2020-05-15T18:49:05.087750Z"
    }
   },
   "outputs": [],
   "source": [
    "combi = train.append(test, ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:49:08.373558Z",
     "start_time": "2020-05-15T18:49:05.578171Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "import contractions\n",
    "combi['tidy_tweet'] = combi['tweet'].apply(\n",
    "    lambda x: [contractions.fix(word) for word in x.split()])\n",
    "\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join(x))\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"@user\", \"\")\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z0-9#]\", \" \")\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace('#', 'hashtag ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:49:08.461653Z",
     "start_time": "2020-05-15T18:49:08.439748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>why do people have kids if they aren't going t...</td>\n",
       "      <td>why do people have kids if they are not going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>booked for @user via @user for @user</td>\n",
       "      <td>booked for  via  for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>have a beautiful saturday! â¤ï¸ . . . . day ...</td>\n",
       "      <td>have a beautiful saturday                 day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user he knows me y'all! ðð this is why...</td>\n",
       "      <td>he knows me you all           this is why tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user on sunday bebs. phones going off. snacks...</td>\n",
       "      <td>on sunday bebs  phones going off  snacks at t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                              tweet  \\\n",
       "0  13785    0.0  why do people have kids if they aren't going t...   \n",
       "1  15932    0.0               booked for @user via @user for @user   \n",
       "2  31788    0.0  have a beautiful saturday! â¤ï¸ . . . . day ...   \n",
       "3  21960    0.0   @user he knows me y'all! ðð this is why...   \n",
       "4   4647    0.0  @user on sunday bebs. phones going off. snacks...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  why do people have kids if they are not going ...  \n",
       "1                              booked for  via  for   \n",
       "2  have a beautiful saturday                 day ...  \n",
       "3   he knows me you all           this is why tru...  \n",
       "4   on sunday bebs  phones going off  snacks at t...  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:49:22.379033Z",
     "start_time": "2020-05-15T18:49:08.509632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.7 s, sys: 1.57 s, total: 10.3 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# function to lemmatize text\n",
    "def lemmatization(texts):\n",
    "    output = []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:49:25.961789Z",
     "start_time": "2020-05-15T18:49:22.497204Z"
    }
   },
   "outputs": [],
   "source": [
    "combi.tidy_tweet = combi.tidy_tweet.apply(lambda x: tknzr.tokenize(x))\n",
    "combi.tidy_tweet = combi.tidy_tweet.apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:49:45.595725Z",
     "start_time": "2020-05-15T18:49:26.057132Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "combi.tidy_tweet = combi.tidy_tweet.apply(lambda x: [wordninja.split(word) for word in x if word not in 'hashtag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:49:45.892761Z",
     "start_time": "2020-05-15T18:49:45.658036Z"
    }
   },
   "outputs": [],
   "source": [
    "combi.tidy_tweet = combi.tidy_tweet.apply(lambda x:[item for sublist in x for item in sublist] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:48:45.437707Z",
     "start_time": "2020-05-15T18:48:45.411076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#handbags #leather #crystals #shop#store#gypsy...</td>\n",
       "      <td>h a n d b a g s l e a t h e r c r y s t a l s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"in a cell phone minute\" 4â #ebook #funny   ...</td>\n",
       "      <td>c e l l p h o n e m i n u t e 4 e b o o k f u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#tb #bestvacations #redsea #entspannung   #fri...</td>\n",
       "      <td>t b b e s t v a c a t i o n s r e d s e a e n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>time for drinks #holiday #cocktails   #corfu @...</td>\n",
       "      <td>t i m e d r i n k s h o l i d a y c o c k t a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#pray4orlando #prayforchristina  what is the w...</td>\n",
       "      <td>p r a y 4 o r l a n d o p r a y f o r c h r i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>gulf steam is moving along eastern seaboard!  ...</td>\n",
       "      <td>g u l f s t e a m m o v i n g a l o n g e a s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tgif #sky #cloudy #rainy   moments times frida...</td>\n",
       "      <td>t g i f s k y c l o u d y r a i n y m o m e n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>my beautiful girl zelda all   after #grooming ...</td>\n",
       "      <td>b e a u t i f u l g i r l z e l d a g r o o m ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user #oscarpistorius shouldn't all murder an...</td>\n",
       "      <td>o s c a r p i s t o r i u s m u r d e r h i j ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#clarity #walk   #positive #sunny #thursday sm...</td>\n",
       "      <td>c l a r i t y w a l k p o s i t i v e s u n n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                              tweet  \\\n",
       "0   3530    0.0  #handbags #leather #crystals #shop#store#gypsy...   \n",
       "1   8671    0.0  \"in a cell phone minute\" 4â #ebook #funny   ...   \n",
       "2  26272    0.0  #tb #bestvacations #redsea #entspannung   #fri...   \n",
       "3  10033    0.0  time for drinks #holiday #cocktails   #corfu @...   \n",
       "4   5990    0.0  #pray4orlando #prayforchristina  what is the w...   \n",
       "5   6297    0.0  gulf steam is moving along eastern seaboard!  ...   \n",
       "6  16096    0.0  tgif #sky #cloudy #rainy   moments times frida...   \n",
       "7  31762    0.0  my beautiful girl zelda all   after #grooming ...   \n",
       "8  21666    0.0   @user #oscarpistorius shouldn't all murder an...   \n",
       "9   9413    0.0  #clarity #walk   #positive #sunny #thursday sm...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  h a n d b a g s l e a t h e r c r y s t a l s ...  \n",
       "1  c e l l p h o n e m i n u t e 4 e b o o k f u ...  \n",
       "2  t b b e s t v a c a t i o n s r e d s e a e n ...  \n",
       "3  t i m e d r i n k s h o l i d a y c o c k t a ...  \n",
       "4  p r a y 4 o r l a n d o p r a y f o r c h r i ...  \n",
       "5  g u l f s t e a m m o v i n g a l o n g e a s ...  \n",
       "6  t g i f s k y c l o u d y r a i n y m o m e n ...  \n",
       "7  b e a u t i f u l g i r l z e l d a g r o o m ...  \n",
       "8  o s c a r p i s t o r i u s m u r d e r h i j ...  \n",
       "9  c l a r i t y w a l k p o s i t i v e s u n n ...  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi.tidy_tweet = lemmatization(combi.tidy_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:04:41.583738Z",
     "start_time": "2020-05-15T18:04:41.552999Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-b7c75ee66761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tidy_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join([text for text in combi['tidy_tweet']])\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:04:41.590984Z",
     "start_time": "2020-05-15T18:04:30.418Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:04:41.599328Z",
     "start_time": "2020-05-15T18:04:31.021Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\n",
    "wordcloud = WordCloud(width=800, height=500,\n",
    "random_state=21, max_font_size=110).generate(negative_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:29:02.875782Z",
     "start_time": "2020-05-12T19:28:17.131642Z"
    }
   },
   "outputs": [],
   "source": [
    "# WORD2VEC\n",
    "from gensim.models import FastText\n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 1\n",
    "\n",
    "f2t_model = FastText(size=W2V_SIZE,\n",
    "                     window=W2V_WINDOW,\n",
    "                     min_count=W2V_MIN_COUNT,\n",
    "                     sg=1,\n",
    "                     hs=1,\n",
    "                     workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:29:03.926475Z",
     "start_time": "2020-05-12T19:29:03.566788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 124 ms, total: 247 ms\n",
      "Wall time: 338 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = [_text.split() for _text in combi.tidy_tweet] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:30:42.030487Z",
     "start_time": "2020-05-12T19:29:04.234376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.7 s, sys: 20.2 s, total: 52.9 s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f2t_model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:09.397560Z",
     "start_time": "2020-05-12T19:32:00.828789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 48888\n",
      "CPU times: user 18min 59s, sys: 8.52 s, total: 19min 8s\n",
      "Wall time: 6min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = f2t_model.wv.vocab.keys()\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)\n",
    "f2t_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:09.575235Z",
     "start_time": "2020-05-12T19:38:09.572473Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_dim = 300 # how big is each word vector\n",
    "max_features = vocab_size # how many unique words to use (i.e num rows in embedding vector)\n",
    "max_seq_len = 50 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:10.103683Z",
     "start_time": "2020-05-12T19:38:09.767278Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train=combi[:len(train)]\n",
    "df_test=combi[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:13.780832Z",
     "start_time": "2020-05-12T19:38:10.335580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  48887\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(combi.tidy_tweet)\n",
    "word_index = tokenizer.word_index\n",
    "word_seq_train = tokenizer.texts_to_sequences(df_train['tidy_tweet'])\n",
    "word_seq_test = tokenizer.texts_to_sequences(df_test['tidy_tweet'])\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "word_seq_train = pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:14.270780Z",
     "start_time": "2020-05-12T19:38:14.200806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (31962, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train.label.tolist())\n",
    "y_train = encoder.transform(df_train.label.tolist())\n",
    "y_train = y_train.reshape(-1,1)\n",
    "print(\"y_train\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:14.562989Z",
     "start_time": "2020-05-12T19:38:14.536351Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/pythonenv/default/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_test.drop('label',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:14.840354Z",
     "start_time": "2020-05-12T19:38:14.818988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>Hashtag studiolife Hashtag aislife Hashtag req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>hashtag white hashtag supremacists want everyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>safe ways to heal your Hashtag acne Hashtag al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>rd Hashtag bihday to my amazing hilarious Hash...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "31962  31963  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963  31964   @user #white #supremacists want everyone to s...   \n",
       "31964  31965  safe ways to heal your #acne!!    #altwaystohe...   \n",
       "31965  31966  is the hp and the cursed child book up for res...   \n",
       "31966  31967    3rd #bihday to my amazing, hilarious #nephew...   \n",
       "\n",
       "                                              tidy_tweet  \n",
       "31962  Hashtag studiolife Hashtag aislife Hashtag req...  \n",
       "31963  hashtag white hashtag supremacists want everyo...  \n",
       "31964  safe ways to heal your Hashtag acne Hashtag al...  \n",
       "31965  is the hp and the cursed child book up for res...  \n",
       "31966  rd Hashtag bihday to my amazing hilarious Hash...  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:15.389542Z",
     "start_time": "2020-05-12T19:38:15.069671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(max_features, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = f2t_model.wv.get_vector(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:15.608605Z",
     "start_time": "2020-05-12T19:38:15.585127Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words, 300, weights=[embedding_matrix], input_length=max_seq_len, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:15.831886Z",
     "start_time": "2020-05-12T19:38:15.816926Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:16.146231Z",
     "start_time": "2020-05-12T19:38:16.139340Z"
    }
   },
   "outputs": [],
   "source": [
    "# build network\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "class Swish(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Swish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'swish'\n",
    "        \n",
    "def swish(x):\n",
    "    return K.sigmoid(x) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:26.365337Z",
     "start_time": "2020-05-12T19:38:16.371099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 300)           14666400  \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 50, 512)           1140736   \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 50, 128)           65664     \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               640100    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 16,513,401\n",
      "Trainable params: 1,846,801\n",
      "Non-trainable params: 14,666,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "get_custom_objects().update({'swish': Swish(swish)})\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2,return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(128, use_bias=True)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('swish'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:26.621694Z",
     "start_time": "2020-05-12T19:38:26.545295Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Nadam',metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:38:26.823668Z",
     "start_time": "2020-05-12T19:38:26.816273Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = 'ModelCheckpoint/Bi_LSTM_f2t.{epoch:02d}-{val_accuracy:.3f}.hdf5'\n",
    "callbacks = [ReduceLROnPlateau(monitor='val_accuracy', patience=5, cooldown=5, verbose=1, factor=0.5),\n",
    "             TensorBoard(log_dir='./Graph', write_graph=True,\n",
    "                         write_images=True),\n",
    "             ModelCheckpoint(filepath, monitor='val_accuracy',\n",
    "                             verbose=1, mode='max',save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T01:43:52.397797Z",
     "start_time": "2020-05-12T19:38:27.010950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28765 samples, validate on 3197 samples\n",
      "Epoch 1/100\n",
      "28765/28765 [==============================] - 253s 9ms/step - loss: 0.1712 - accuracy: 0.9396 - val_loss: 0.1526 - val_accuracy: 0.9478\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.94776, saving model to ModelCheckpoint/Bi_LSTM_f2t.01-0.948.hdf5\n",
      "Epoch 2/100\n",
      "28765/28765 [==============================] - 238s 8ms/step - loss: 0.1136 - accuracy: 0.9598 - val_loss: 0.1091 - val_accuracy: 0.9640\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.94776 to 0.96403, saving model to ModelCheckpoint/Bi_LSTM_f2t.02-0.964.hdf5\n",
      "Epoch 3/100\n",
      "28765/28765 [==============================] - 235s 8ms/step - loss: 0.0983 - accuracy: 0.9662 - val_loss: 0.1147 - val_accuracy: 0.9625\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.96403\n",
      "Epoch 4/100\n",
      "28765/28765 [==============================] - 240s 8ms/step - loss: 0.0847 - accuracy: 0.9704 - val_loss: 0.1062 - val_accuracy: 0.9675\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.96403 to 0.96747, saving model to ModelCheckpoint/Bi_LSTM_f2t.04-0.967.hdf5\n",
      "Epoch 5/100\n",
      "28765/28765 [==============================] - 236s 8ms/step - loss: 0.0711 - accuracy: 0.9740 - val_loss: 0.1178 - val_accuracy: 0.9643\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.96747\n",
      "Epoch 6/100\n",
      "28765/28765 [==============================] - 242s 8ms/step - loss: 0.0543 - accuracy: 0.9807 - val_loss: 0.1205 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.96747\n",
      "Epoch 7/100\n",
      "28765/28765 [==============================] - 243s 8ms/step - loss: 0.0406 - accuracy: 0.9852 - val_loss: 0.1659 - val_accuracy: 0.9678\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.96747 to 0.96778, saving model to ModelCheckpoint/Bi_LSTM_f2t.07-0.968.hdf5\n",
      "Epoch 8/100\n",
      "28765/28765 [==============================] - 238s 8ms/step - loss: 0.0314 - accuracy: 0.9885 - val_loss: 0.1616 - val_accuracy: 0.9653\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.96778\n",
      "Epoch 9/100\n",
      "28765/28765 [==============================] - 237s 8ms/step - loss: 0.0260 - accuracy: 0.9912 - val_loss: 0.1771 - val_accuracy: 0.9675\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.96778\n",
      "Epoch 10/100\n",
      "28765/28765 [==============================] - 222s 8ms/step - loss: 0.0220 - accuracy: 0.9921 - val_loss: 0.1572 - val_accuracy: 0.9656\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.96778\n",
      "Epoch 11/100\n",
      "28765/28765 [==============================] - 224s 8ms/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 0.1744 - val_accuracy: 0.9609\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.96778\n",
      "Epoch 12/100\n",
      "28765/28765 [==============================] - 222s 8ms/step - loss: 0.0143 - accuracy: 0.9952 - val_loss: 0.2360 - val_accuracy: 0.9697\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.96778 to 0.96966, saving model to ModelCheckpoint/Bi_LSTM_f2t.12-0.970.hdf5\n",
      "Epoch 13/100\n",
      "28765/28765 [==============================] - 221s 8ms/step - loss: 0.0136 - accuracy: 0.9948 - val_loss: 0.2035 - val_accuracy: 0.9687\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.96966\n",
      "Epoch 14/100\n",
      "28765/28765 [==============================] - 221s 8ms/step - loss: 0.0126 - accuracy: 0.9959 - val_loss: 0.2308 - val_accuracy: 0.9690\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.96966\n",
      "Epoch 15/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0104 - accuracy: 0.9960 - val_loss: 0.2326 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.96966 to 0.97060, saving model to ModelCheckpoint/Bi_LSTM_f2t.15-0.971.hdf5\n",
      "Epoch 16/100\n",
      "28765/28765 [==============================] - 223s 8ms/step - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.2561 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.97060 to 0.97091, saving model to ModelCheckpoint/Bi_LSTM_f2t.16-0.971.hdf5\n",
      "Epoch 17/100\n",
      "28765/28765 [==============================] - 224s 8ms/step - loss: 0.0107 - accuracy: 0.9961 - val_loss: 0.1982 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.97091 to 0.97122, saving model to ModelCheckpoint/Bi_LSTM_f2t.17-0.971.hdf5\n",
      "Epoch 18/100\n",
      "28765/28765 [==============================] - 223s 8ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.2188 - val_accuracy: 0.9675\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.97122\n",
      "Epoch 19/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 0.2346 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.97122\n",
      "Epoch 20/100\n",
      "28765/28765 [==============================] - 224s 8ms/step - loss: 0.0090 - accuracy: 0.9965 - val_loss: 0.2433 - val_accuracy: 0.9731\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.97122 to 0.97310, saving model to ModelCheckpoint/Bi_LSTM_f2t.20-0.973.hdf5\n",
      "Epoch 21/100\n",
      "28765/28765 [==============================] - 228s 8ms/step - loss: 0.0096 - accuracy: 0.9967 - val_loss: 0.2194 - val_accuracy: 0.9690\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.97310\n",
      "Epoch 22/100\n",
      "28765/28765 [==============================] - 226s 8ms/step - loss: 0.0064 - accuracy: 0.9977 - val_loss: 0.2293 - val_accuracy: 0.9659\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.97310\n",
      "Epoch 23/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.2583 - val_accuracy: 0.9678\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.97310\n",
      "Epoch 24/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 0.2328 - val_accuracy: 0.9672\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97310\n",
      "Epoch 25/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.2596 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.97310\n",
      "Epoch 26/100\n",
      "28765/28765 [==============================] - 226s 8ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.2379 - val_accuracy: 0.9693\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.97310\n",
      "Epoch 27/100\n",
      "28765/28765 [==============================] - 227s 8ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.2669 - val_accuracy: 0.9700\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.97310\n",
      "Epoch 28/100\n",
      "28765/28765 [==============================] - 228s 8ms/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.2621 - val_accuracy: 0.9700\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97310\n",
      "Epoch 29/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.2737 - val_accuracy: 0.9697\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.97310\n",
      "Epoch 30/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.2960 - val_accuracy: 0.9697\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.97310\n",
      "Epoch 31/100\n",
      "28765/28765 [==============================] - 228s 8ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.2988 - val_accuracy: 0.9731\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.97310\n",
      "Epoch 32/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.2645 - val_accuracy: 0.9700\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.97310\n",
      "Epoch 33/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2869 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.97310\n",
      "Epoch 34/100\n",
      "28765/28765 [==============================] - 224s 8ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.2808 - val_accuracy: 0.9700\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97310\n",
      "Epoch 35/100\n",
      "28765/28765 [==============================] - 225s 8ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.2844 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.97310\n",
      "Epoch 36/100\n",
      "28765/28765 [==============================] - 226s 8ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.2968 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97310\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28765/28765 [==============================] - 213s 7ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.3069 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.97310\n",
      "Epoch 38/100\n",
      "28765/28765 [==============================] - 212s 7ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.3027 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97310\n",
      "Epoch 39/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.2828 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97310\n",
      "Epoch 40/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 9.1209e-04 - accuracy: 0.9998 - val_loss: 0.3167 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97310\n",
      "Epoch 41/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 7.1276e-04 - accuracy: 0.9999 - val_loss: 0.3229 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97310\n",
      "Epoch 42/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 7.2926e-04 - accuracy: 0.9998 - val_loss: 0.3281 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97310\n",
      "Epoch 43/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 9.5872e-04 - accuracy: 0.9998 - val_loss: 0.3326 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97310\n",
      "Epoch 44/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 6.2925e-04 - accuracy: 0.9998 - val_loss: 0.3432 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97310\n",
      "Epoch 45/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 7.3061e-04 - accuracy: 0.9998 - val_loss: 0.3404 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97310\n",
      "Epoch 46/100\n",
      "28765/28765 [==============================] - 214s 7ms/step - loss: 6.0665e-04 - accuracy: 0.9998 - val_loss: 0.3434 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97310\n",
      "Epoch 47/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 6.1228e-04 - accuracy: 0.9998 - val_loss: 0.3365 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97310\n",
      "Epoch 48/100\n",
      "28765/28765 [==============================] - 216s 8ms/step - loss: 7.9300e-04 - accuracy: 0.9997 - val_loss: 0.3428 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97310\n",
      "Epoch 49/100\n",
      "28765/28765 [==============================] - 214s 7ms/step - loss: 6.3139e-04 - accuracy: 0.9998 - val_loss: 0.3435 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97310\n",
      "Epoch 50/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 4.9347e-04 - accuracy: 0.9998 - val_loss: 0.3458 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97310\n",
      "Epoch 51/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.3486 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97310\n",
      "Epoch 52/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 4.4616e-04 - accuracy: 0.9999 - val_loss: 0.3439 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97310\n",
      "Epoch 53/100\n",
      "28765/28765 [==============================] - 214s 7ms/step - loss: 7.5614e-04 - accuracy: 0.9998 - val_loss: 0.3375 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97310\n",
      "Epoch 54/100\n",
      "28765/28765 [==============================] - 214s 7ms/step - loss: 3.9536e-04 - accuracy: 0.9998 - val_loss: 0.3394 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97310\n",
      "Epoch 55/100\n",
      "28765/28765 [==============================] - 214s 7ms/step - loss: 8.5463e-04 - accuracy: 0.9997 - val_loss: 0.3430 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97310\n",
      "Epoch 56/100\n",
      "28765/28765 [==============================] - 214s 7ms/step - loss: 3.2340e-04 - accuracy: 0.9999 - val_loss: 0.3428 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97310\n",
      "Epoch 57/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 4.9990e-04 - accuracy: 0.9999 - val_loss: 0.3355 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97310\n",
      "Epoch 58/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 6.5250e-04 - accuracy: 0.9999 - val_loss: 0.3347 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97310\n",
      "Epoch 59/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 4.9016e-04 - accuracy: 0.9998 - val_loss: 0.3432 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97310\n",
      "Epoch 60/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 3.5497e-04 - accuracy: 0.9999 - val_loss: 0.3416 - val_accuracy: 0.9697\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97310\n",
      "Epoch 61/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 3.5989e-04 - accuracy: 0.9999 - val_loss: 0.3480 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.97310\n",
      "Epoch 62/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 3.6010e-04 - accuracy: 0.9999 - val_loss: 0.3472 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.97310\n",
      "Epoch 63/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 6.6700e-04 - accuracy: 0.9998 - val_loss: 0.3467 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.97310\n",
      "Epoch 64/100\n",
      "28765/28765 [==============================] - 218s 8ms/step - loss: 8.7613e-05 - accuracy: 1.0000 - val_loss: 0.3489 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.97310\n",
      "Epoch 65/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 4.2993e-04 - accuracy: 0.9999 - val_loss: 0.3474 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.97310\n",
      "Epoch 66/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 3.2302e-04 - accuracy: 0.9999 - val_loss: 0.3475 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.97310\n",
      "Epoch 67/100\n",
      "28765/28765 [==============================] - 216s 7ms/step - loss: 3.1658e-04 - accuracy: 0.9999 - val_loss: 0.3555 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.97310\n",
      "Epoch 68/100\n",
      "28765/28765 [==============================] - 216s 7ms/step - loss: 2.8564e-04 - accuracy: 0.9999 - val_loss: 0.3522 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.97310\n",
      "Epoch 69/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 2.4217e-04 - accuracy: 0.9999 - val_loss: 0.3488 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.97310\n",
      "Epoch 70/100\n",
      "28765/28765 [==============================] - 217s 8ms/step - loss: 2.0123e-04 - accuracy: 0.9999 - val_loss: 0.3501 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.97310\n",
      "Epoch 71/100\n",
      "28765/28765 [==============================] - 216s 8ms/step - loss: 5.2068e-04 - accuracy: 0.9998 - val_loss: 0.3461 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.97310\n",
      "Epoch 72/100\n",
      "28765/28765 [==============================] - 233s 8ms/step - loss: 4.4458e-04 - accuracy: 0.9998 - val_loss: 0.3465 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.97310\n",
      "Epoch 73/100\n",
      "28765/28765 [==============================] - 216s 8ms/step - loss: 4.0371e-04 - accuracy: 0.9998 - val_loss: 0.3534 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.97310\n",
      "Epoch 74/100\n",
      "28765/28765 [==============================] - 218s 8ms/step - loss: 4.9249e-04 - accuracy: 0.9998 - val_loss: 0.3499 - val_accuracy: 0.9715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.97310\n",
      "Epoch 75/100\n",
      "28765/28765 [==============================] - 212s 7ms/step - loss: 2.8566e-04 - accuracy: 1.0000 - val_loss: 0.3478 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.97310\n",
      "Epoch 76/100\n",
      "28765/28765 [==============================] - 210s 7ms/step - loss: 2.8741e-04 - accuracy: 0.9999 - val_loss: 0.3495 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.97310\n",
      "Epoch 77/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 4.3255e-04 - accuracy: 0.9998 - val_loss: 0.3516 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.97310\n",
      "Epoch 78/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 2.1635e-04 - accuracy: 0.9999 - val_loss: 0.3504 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.97310\n",
      "Epoch 79/100\n",
      "28765/28765 [==============================] - 212s 7ms/step - loss: 2.1200e-04 - accuracy: 1.0000 - val_loss: 0.3545 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.97310\n",
      "Epoch 80/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 3.6660e-04 - accuracy: 0.9999 - val_loss: 0.3552 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.97310\n",
      "Epoch 81/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 1.7973e-04 - accuracy: 0.9999 - val_loss: 0.3519 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.97310\n",
      "Epoch 82/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 3.9935e-04 - accuracy: 0.9999 - val_loss: 0.3475 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.97310\n",
      "Epoch 83/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 1.8798e-04 - accuracy: 0.9999 - val_loss: 0.3522 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.97310\n",
      "Epoch 84/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 2.5767e-04 - accuracy: 0.9999 - val_loss: 0.3516 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.97310\n",
      "Epoch 85/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 1.7387e-04 - accuracy: 1.0000 - val_loss: 0.3508 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.97310\n",
      "Epoch 86/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 3.3791e-04 - accuracy: 0.9999 - val_loss: 0.3537 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.97310\n",
      "Epoch 87/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 1.7564e-04 - accuracy: 1.0000 - val_loss: 0.3576 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.97310\n",
      "Epoch 88/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 1.3897e-04 - accuracy: 1.0000 - val_loss: 0.3537 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.97310\n",
      "Epoch 89/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 2.1612e-04 - accuracy: 0.9999 - val_loss: 0.3567 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.97310\n",
      "Epoch 90/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 8.9908e-05 - accuracy: 1.0000 - val_loss: 0.3554 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.97310\n",
      "Epoch 91/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 1.7756e-04 - accuracy: 0.9999 - val_loss: 0.3514 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.97310\n",
      "Epoch 92/100\n",
      "28765/28765 [==============================] - 212s 7ms/step - loss: 2.9748e-04 - accuracy: 0.9999 - val_loss: 0.3540 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.97310\n",
      "Epoch 93/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 2.8203e-04 - accuracy: 0.9999 - val_loss: 0.3554 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.97310\n",
      "Epoch 94/100\n",
      "28765/28765 [==============================] - 212s 7ms/step - loss: 2.1684e-04 - accuracy: 0.9999 - val_loss: 0.3554 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.97310\n",
      "Epoch 95/100\n",
      "28765/28765 [==============================] - 211s 7ms/step - loss: 3.2981e-04 - accuracy: 0.9999 - val_loss: 0.3559 - val_accuracy: 0.9715\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.97310\n",
      "Epoch 96/100\n",
      "28765/28765 [==============================] - 212s 7ms/step - loss: 2.1768e-04 - accuracy: 0.9999 - val_loss: 0.3554 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.97310\n",
      "Epoch 97/100\n",
      "28765/28765 [==============================] - 212s 7ms/step - loss: 2.0997e-04 - accuracy: 0.9999 - val_loss: 0.3579 - val_accuracy: 0.9718\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.97310\n",
      "Epoch 98/100\n",
      "28765/28765 [==============================] - 215s 7ms/step - loss: 1.5960e-04 - accuracy: 1.0000 - val_loss: 0.3570 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.97310\n",
      "Epoch 99/100\n",
      "28765/28765 [==============================] - 213s 7ms/step - loss: 2.4475e-04 - accuracy: 0.9999 - val_loss: 0.3555 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.97310\n",
      "Epoch 100/100\n",
      "28765/28765 [==============================] - 212s 7ms/step - loss: 2.1308e-04 - accuracy: 0.9999 - val_loss: 0.3577 - val_accuracy: 0.9712\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.97310\n",
      "CPU times: user 17h 27min 1s, sys: 2h 6min 31s, total: 19h 33min 32s\n",
      "Wall time: 6h 5min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(word_seq_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T19:09:32.505217Z",
     "start_time": "2020-05-12T19:09:20.721121Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('ModelCheckpoint/Bi_LSTM_f2t.11-0.974.hdf5',custom_objects={'swish': Swish(swish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:31:04.212950Z",
     "start_time": "2020-05-13T04:30:08.874508Z"
    }
   },
   "outputs": [],
   "source": [
    "pred=model.predict(word_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:31:04.424008Z",
     "start_time": "2020-05-13T04:31:04.419312Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sentiment(prediction_prob):\n",
    "    sentiment = []\n",
    "    for idx, val in enumerate(prediction_prob):\n",
    "        if val>0.5:\n",
    "            sentiment.append(1)\n",
    "        else:\n",
    "            sentiment.append(0)\n",
    "    print('Done')\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:31:04.657165Z",
     "start_time": "2020-05-13T04:31:04.624162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "sentiment = decode_sentiment(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:31:04.894008Z",
     "start_time": "2020-05-13T04:31:04.875557Z"
    }
   },
   "outputs": [],
   "source": [
    "Submission = pd.DataFrame()\n",
    "Submission['id']=df_test['id']\n",
    "Submission['label']=sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T04:31:05.110545Z",
     "start_time": "2020-05-13T04:31:05.070248Z"
    }
   },
   "outputs": [],
   "source": [
    "Submission.to_csv('Submission_BiLSTM_Fasttext.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T17:50:35.230492Z",
     "start_time": "2020-05-15T17:50:26.786819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycontractions\n",
      "  Downloading pycontractions-2.0.1-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: gensim>=2.0 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from pycontractions) (3.8.1)\n",
      "Collecting language-check>=1.0\n",
      "  Downloading language-check-1.1.tar.gz (33 kB)\n",
      "Requirement already satisfied: pyemd>=0.4.4 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from pycontractions) (0.5.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from gensim>=2.0->pycontractions) (1.8.4)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from gensim>=2.0->pycontractions) (1.18.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from gensim>=2.0->pycontractions) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from gensim>=2.0->pycontractions) (1.14.0)\n",
      "Requirement already satisfied: boto>=2.32 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.49.0)\n",
      "Requirement already satisfied: requests in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.23.0)\n",
      "Requirement already satisfied: boto3 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.9.250)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.9.11)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.250 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.12.250)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.2.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.250->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/subir/pythonenv/default/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.250->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.15.2)\n",
      "Building wheels for collected packages: language-check\n",
      "  Building wheel for language-check (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Users/subir/pythonenv/default/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-install-eaoj88xr/language-check/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-install-eaoj88xr/language-check/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-wheel-y3m8p0bb\n",
      "       cwd: /private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-install-eaoj88xr/language-check/\n",
      "  Complete output (4 lines):\n",
      "  Could not parse Java version from \"\"\"openjdk version \"14.0.1\" 2020-04-14\n",
      "  OpenJDK Runtime Environment (build 14.0.1+7)\n",
      "  OpenJDK 64-Bit Server VM (build 14.0.1+7, mixed mode, sharing)\n",
      "  \"\"\".\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for language-check\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for language-check\n",
      "Failed to build language-check\n",
      "Installing collected packages: language-check, pycontractions\n",
      "    Running setup.py install for language-check ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/subir/pythonenv/default/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-install-eaoj88xr/language-check/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-install-eaoj88xr/language-check/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-record-32s4ygzo/install-record.txt --single-version-externally-managed --compile --install-headers /Users/subir/pythonenv/default/include/site/python3.7/language-check\n",
      "         cwd: /private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-install-eaoj88xr/language-check/\n",
      "    Complete output (4 lines):\n",
      "    Could not parse Java version from \"\"\"openjdk version \"14.0.1\" 2020-04-14\n",
      "    OpenJDK Runtime Environment (build 14.0.1+7)\n",
      "    OpenJDK 64-Bit Server VM (build 14.0.1+7, mixed mode, sharing)\n",
      "    \"\"\".\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /Users/subir/pythonenv/default/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-install-eaoj88xr/language-check/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-install-eaoj88xr/language-check/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/5m/h_g_92_s1d11s4pd5cbhs48m0000gn/T/pip-record-32s4ygzo/install-record.txt --single-version-externally-managed --compile --install-headers /Users/subir/pythonenv/default/include/site/python3.7/language-check Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T18:38:18.605818Z",
     "start_time": "2020-05-15T18:38:11.370615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordninja\n",
      "  Downloading wordninja-2.0.0.tar.gz (541 kB)\n",
      "\u001b[K     |████████████████████████████████| 541 kB 894 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: wordninja\n",
      "  Building wheel for wordninja (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541552 sha256=fab1f97a6616e6c87a9fea60e9ccdaa065ccc33cee951ba7be2e576da3083baa\n",
      "  Stored in directory: /Users/subir/Library/Caches/pip/wheels/dd/3f/eb/a2692e3d2b9deb1487b09ba4967dd6920bd5032bfd9ff7acfc\n",
      "Successfully built wordninja\n",
      "Installing collected packages: wordninja\n",
      "Successfully installed wordninja-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
