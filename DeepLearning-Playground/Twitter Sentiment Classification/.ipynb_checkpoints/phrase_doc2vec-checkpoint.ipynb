{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing ====>\n",
      "Train set has total 22373 entries with 92.93 % positive, 7.07% negative\n",
      "Validation set has total 4794 entries with 92.89 % positive, 7.11% negative\n",
      "Test set has total 4795 entries with 93.33 % positive, 6.67% negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "\n",
    "\n",
    "train=pd.read_csv('train_E6oV3lV.csv')\n",
    "test=pd.read_csv('test_tweets_anuFYb8.csv')\n",
    "\n",
    "target=train.label\n",
    "train=train.drop('label',1)\n",
    "data=train.append(test)\n",
    "tweet =data.tweet\n",
    "\n",
    "\n",
    "# pre-processing\n",
    "print(\"Text Preprocessing ====>\")\n",
    "data['tweet']=data['tweet'].apply(lambda x: x.lower())\n",
    "data['tweet']=data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['tweet']=data['tweet'].apply(lambda x: x.replace('user',''))\n",
    "stop = stopwords.words('english')\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "x = data['tweet'][:len(train)]\n",
    "y = target\n",
    "\n",
    "x_train, x_validation_and_test , y_train , y_validation_and_test = train_test_split(x, y,test_size=.3,random_state= SEED)\n",
    "\n",
    "x_validation , x_test , y_validation , y_test = train_test_split(x_validation_and_test,y_validation_and_test,test_size=0.5,random_state=SEED)\n",
    "\n",
    "\n",
    "print(\"Train set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_train),(len(x_train[y_train==0])/(len(x_train)*1.))*100,\n",
    "                                                                                          (len(x_train[y_train == 1]) / (len(x_train) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Validation set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_validation),(len(x_validation[y_validation==0])/(len(x_validation)*1.))*100,\n",
    "                                                                                          (len(x_validation[y_validation == 1]) / (len(x_validation) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_test),(len(x_test[y_test==0])/(len(x_test)*1.))*100,\n",
    "                                                                                          (len(x_test[y_test == 1]) / (len(x_test) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def labelize_tweets_ug(tweets,label):\n",
    "\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i , t in zip(tweets.index,tweets):\n",
    "        result.append(TaggedDocument(t.split(),[prefix + '_%s' % i ]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x,'all')\n",
    "\n",
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n= 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "\n",
    "    return vecs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases , Phraser\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "tokenized_train = [t.split() for t in all_x]\n",
    "phrases = Phrases(tokenized_train)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_bg(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(bigram[t.split()], [prefix + '_%s' % i]))\n",
    "    return result\n",
    "  \n",
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v_bg = labelize_tweets_bg(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 1936056.27it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2501695.27it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2743499.19it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2602569.30it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2648693.90it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2798946.56it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2764146.57it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2837333.74it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2711865.20it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2786844.01it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2946596.28it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3042562.46it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2380635.47it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2890122.76it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2697893.83it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2865290.45it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2650893.68it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2929916.83it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2765857.44it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2860582.63it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2754887.68it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2587599.30it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2792940.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2896679.87it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2888877.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2762267.05it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2773525.28it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2753529.65it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2977089.59it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2875061.00it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2443600.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9390905298289528\n"
     ]
    }
   ],
   "source": [
    "# Distributed Bag of Words\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model_ug_dbow = Doc2Vec(dm=0,vector_size=100,negative=5,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v_bg)])\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg),epochs=1)\n",
    "    model_ug_dbow.alpha -=0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha\n",
    "\n",
    "train_vecs_dbow = get_vectors(model_ug_dbow, x_train, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow, y_train)\n",
    "print('score',clf.score(validation_vecs_dbow,y_validation))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 2321758.65it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2949903.06it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2699686.74it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2936848.96it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2748054.54it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2820499.57it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2775420.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2865535.44it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2762039.40it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2511584.69it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2708139.96it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2829249.82it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2567872.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2805683.11it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2629523.06it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2787249.61it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2933122.08it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2811331.54it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2690421.94it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2871673.72it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2407440.86it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2938458.30it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2936334.34it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2894053.46it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2711865.20it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2844316.91it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2845101.65it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2916277.15it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2985975.24it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2427098.24it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2781235.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.928869420108469\n"
     ]
    }
   ],
   "source": [
    "# Distributed Memory Concatenation\n",
    "\n",
    "model_ug_dmc = Doc2Vec(dm=1,dm_concat=1,vector_size=100,negative=5,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_dmc.build_vocab([x for x in tqdm(all_x_w2v_bg)])\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg),epochs=1)\n",
    "    model_ug_dmc.alpha -=0.002\n",
    "    model_ug_dmc.min_alpha = model_ug_dmc.alpha\n",
    "\n",
    "train_vecs_dmc = get_vectors(model_ug_dmc, x_train, 100)\n",
    "validation_vecs_dmc = get_vectors(model_ug_dmc, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc, y_train)\n",
    "print('score',clf.score(validation_vecs_dmc,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 2669474.59it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2870935.74it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2703388.74it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2776224.83it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2711865.20it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2734600.99it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2793173.13it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2597274.91it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2867496.83it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2918562.79it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2712523.66it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2804391.87it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2934470.37it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2696103.30it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2830982.48it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2859362.35it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2813809.89it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2971282.96it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2905343.17it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2965498.93it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2944137.23it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2752003.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Distributed Memory Mean\n",
    "\n",
    "model_ug_dmm = Doc2Vec(dm=1,dm_mean=1,vector_size=100,negative=5,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v_bg)])\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg),epochs=1)\n",
    "    model_ug_dmm.alpha -=0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha\n",
    "\n",
    "train_vecs_dmm = get_vectors(model_ug_dmm, x_train, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, y_train)\n",
    "print('score',clf.score(validation_vecs_dmm,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
