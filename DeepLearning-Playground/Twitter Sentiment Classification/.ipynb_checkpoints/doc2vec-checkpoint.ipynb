{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing ====>\n",
      "Train set has total 22373 entries with 92.93 % positive, 7.07% negative\n",
      "Validation set has total 4794 entries with 92.89 % positive, 7.11% negative\n",
      "Test set has total 4795 entries with 93.33 % positive, 6.67% negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "\n",
    "\n",
    "train=pd.read_csv('train_E6oV3lV.csv')\n",
    "test=pd.read_csv('test_tweets_anuFYb8.csv')\n",
    "\n",
    "target=train.label\n",
    "train=train.drop('label',1)\n",
    "data=train.append(test)\n",
    "tweet =data.tweet\n",
    "\n",
    "\n",
    "# pre-processing\n",
    "print(\"Text Preprocessing ====>\")\n",
    "data['tweet']=data['tweet'].apply(lambda x: x.lower())\n",
    "data['tweet']=data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['tweet']=data['tweet'].apply(lambda x: x.replace('user',''))\n",
    "stop = stopwords.words('english')\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "x = data['tweet'][:len(train)]\n",
    "y = target\n",
    "\n",
    "x_train, x_validation_and_test , y_train , y_validation_and_test = train_test_split(x, y,test_size=.3,random_state= SEED)\n",
    "\n",
    "x_validation , x_test , y_validation , y_test = train_test_split(x_validation_and_test,y_validation_and_test,test_size=0.5,random_state=SEED)\n",
    "\n",
    "\n",
    "print(\"Train set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_train),(len(x_train[y_train==0])/(len(x_train)*1.))*100,\n",
    "                                                                                          (len(x_train[y_train == 1]) / (len(x_train) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Validation set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_validation),(len(x_validation[y_validation==0])/(len(x_validation)*1.))*100,\n",
    "                                                                                          (len(x_validation[y_validation == 1]) / (len(x_validation) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_test),(len(x_test[y_test==0])/(len(x_test)*1.))*100,\n",
    "                                                                                          (len(x_test[y_test == 1]) / (len(x_test) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def labelize_tweets_ug(tweets,label):\n",
    "\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i , t in zip(tweets.index,tweets):\n",
    "        result.append(TaggedDocument(t.split(),[prefix + '_%s' % i ]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x,'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 1643798.52it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1984227.00it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2039437.49it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2007793.20it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2078423.94it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2215767.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1937819.38it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1858385.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1945468.52it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1712570.99it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1984080.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1959888.66it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2045692.86it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2061896.80it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2051107.64it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2031279.37it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2021752.19it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2169404.39it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2043852.73it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2074371.68it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2087648.44it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2117791.89it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1995153.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2103700.97it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2102612.13it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2027286.05it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2276458.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2105948.20it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2123292.91it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1972867.87it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2061167.66it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ee82085c6d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel_ug_dbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ug_dbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_vecs_dbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ug_dbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mvalidation_vecs_dbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ug_dbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "# Distributed Bag of Words\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cores = multiprocessing.cpu_count()\n",
    " \n",
    "model_ug_cbow = Word2Vec(sg=0,size=100,negative=5,window=2,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])\n",
    "\n",
    "\n",
    "model_ug_sg = Word2Vec(sg=1,size=100,negative=5,window=2,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])\n",
    "\n",
    "def get_w2v_mean(tweet, size):\n",
    "    vecs = np.zeros(size).reshape((1,size))\n",
    "    count = 0\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_cbow[word],model_ug_sg[word]).reshape((1,size))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v),epochs=1)\n",
    "    model_ug_cbow.alpha -=0.002\n",
    "    model_ug_cbow.min_alpha = model_ug_cbow.alpha\n",
    "\n",
    "train_vecs_dbow = get_vectors(model_ug_dbow, x_train, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow, y_train)\n",
    "print('score',clf.score(validation_vecs_dbow,y_validation))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 2139900.47it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2780543.51it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2928316.83it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2566643.27it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2174118.88it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3118868.96it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2593004.73it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2675281.27it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2611644.90it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2890932.99it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2509609.95it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3083076.78it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2622014.25it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2581719.07it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2803394.91it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2453483.61it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2535478.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2996319.81it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2966286.33it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3025942.81it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2741871.93it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3124611.79it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2480586.65it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2815287.17it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2926910.28it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2821746.29it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2458658.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2880497.30it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2920788.37it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3036497.87it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2678220.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.928869420108469\n"
     ]
    }
   ],
   "source": [
    "# Distributed Memory Concatenation\n",
    "\n",
    "model_ug_dmc = Doc2Vec(dm=1,dm_concat=1,vector_size=100,negative=5,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_dmc.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v),epochs=1)\n",
    "    model_ug_dmc.alpha -=0.002\n",
    "    model_ug_dmc.min_alpha = model_ug_dmc.alpha\n",
    "\n",
    "train_vecs_dmc = get_vectors(model_ug_dmc, x_train, 100)\n",
    "validation_vecs_dmc = get_vectors(model_ug_dmc, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc, y_train)\n",
    "print('score',clf.score(validation_vecs_dmc,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 2288895.91it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2653307.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2588198.79it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2621552.78it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2470438.49it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2599793.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2306221.41it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2737392.94it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2661102.18it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2832537.70it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2875862.80it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2487767.82it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2674427.33it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2752455.49it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2915199.07it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2453169.33it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2972402.93it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2853519.46it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2919643.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2621809.13it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2734154.81it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2975107.51it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3006264.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2716260.98it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2813809.89it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2586750.50it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2424113.86it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2650893.68it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2543898.15it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3052052.28it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2914945.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9309553608677513\n"
     ]
    }
   ],
   "source": [
    "# Distributed Memory Mean\n",
    "\n",
    "model_ug_dmm = Doc2Vec(dm=1,dm_mean=1,vector_size=100,negative=5,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v),epochs=1)\n",
    "    model_ug_dmm.alpha -=0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha\n",
    "\n",
    "train_vecs_dmm = get_vectors(model_ug_dmm, x_train, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, y_train)\n",
    "print('score',clf.score(validation_vecs_dmm,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9413850646641635"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_train, 200)\n",
    "validation_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_validation, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc, y_train)\n",
    "clf.score(validation_vecs_dbow_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9397163120567376"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_train, 200)\n",
    "validation_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_validation, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm, y_train)\n",
    "clf.score(validation_vecs_dbow_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9309553608677513"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dmm_dmc = get_concat_vectors(model_ug_dmm,model_ug_dmc, x_train, 200)\n",
    "validation_vecs_dmm_dmc = get_concat_vectors(model_ug_dmm,model_ug_dmc, x_validation, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm_dmc, y_train)\n",
    "clf.score(validation_vecs_dmm_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
