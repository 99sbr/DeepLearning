{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/subir/anaconda3/envs/DeepLearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing ====>\n",
      "Train set has total 22373 entries with 92.93 % positive, 7.07% negative\n",
      "Validation set has total 4794 entries with 92.89 % positive, 7.11% negative\n",
      "Test set has total 4795 entries with 93.33 % positive, 6.67% negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "\n",
    "\n",
    "train=pd.read_csv('train_E6oV3lV.csv')\n",
    "test=pd.read_csv('test_tweets_anuFYb8.csv')\n",
    "\n",
    "target=train.label\n",
    "train=train.drop('label',1)\n",
    "data=train.append(test)\n",
    "tweet =data.tweet\n",
    "\n",
    "\n",
    "# pre-processing\n",
    "print(\"Text Preprocessing ====>\")\n",
    "data['tweet']=data['tweet'].apply(lambda x: x.lower())\n",
    "data['tweet']=data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['tweet']=data['tweet'].apply(lambda x: x.replace('user',''))\n",
    "stop = stopwords.words('english')\n",
    "data['tweet'] = data['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "x = data['tweet'][:len(train)]\n",
    "y = target\n",
    "\n",
    "x_train, x_validation_and_test , y_train , y_validation_and_test = train_test_split(x, y,test_size=.3,random_state= SEED)\n",
    "\n",
    "x_validation , x_test , y_validation , y_test = train_test_split(x_validation_and_test,y_validation_and_test,test_size=0.5,random_state=SEED)\n",
    "\n",
    "\n",
    "print(\"Train set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_train),(len(x_train[y_train==0])/(len(x_train)*1.))*100,\n",
    "                                                                                          (len(x_train[y_train == 1]) / (len(x_train) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Validation set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_validation),(len(x_validation[y_validation==0])/(len(x_validation)*1.))*100,\n",
    "                                                                                          (len(x_validation[y_validation == 1]) / (len(x_validation) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test set has total {0} entries with {1:.2f} % positive, {2:.2f}% negative\".format(len(x_test),(len(x_test[y_test==0])/(len(x_test)*1.))*100,\n",
    "                                                                                          (len(x_test[y_test == 1]) / (len(x_test) * 1.)) * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def labelize_tweets_ug(tweets,label):\n",
    "\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i , t in zip(tweets.index,tweets):\n",
    "        result.append(TaggedDocument(t.split(),[prefix + '_%s' % i ]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x,'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 1383228.38it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2071006.85it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1909554.22it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2180555.06it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2078037.33it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2252778.52it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2148026.67it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1923693.38it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2162615.05it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2059394.50it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1999229.65it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2182756.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2156839.26it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2001139.62it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2235908.14it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1971591.21it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2104559.64it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2120740.11it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1835636.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 1857072.43it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2101359.72it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2149025.26it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2194009.11it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2021508.30it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2149852.37it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2054785.94it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2104295.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2054219.19it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2263925.43it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2161220.47it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2136626.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2078133.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9468085106382979\n",
      "score 0.9518147684605757\n"
     ]
    }
   ],
   "source": [
    "# Distributed Bag of Words\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import scale\n",
    "cores = multiprocessing.cpu_count()\n",
    " \n",
    "model_ug_cbow = Word2Vec(sg=0,size=100,negative=5,window=2,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])\n",
    "\n",
    "\n",
    "model_ug_sg = Word2Vec(sg=1,size=100,negative=5,window=2,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])\n",
    "\n",
    "def get_w2v_mean(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1,size))\n",
    "    count = 0\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_cbow[word],model_ug_sg[word]).reshape((1,size))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def get_w2v_sum(tweet, size):\n",
    "    vec = np.zeros(size).reshape((1,size))\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += np.append(model_ug_cbow[word],model_ug_sg[word]).reshape((1,size))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return vec\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v),epochs=1)\n",
    "    model_ug_cbow.alpha -=0.002\n",
    "    model_ug_cbow.min_alpha = model_ug_cbow.alpha\n",
    "\n",
    "train_vecs_cbow_sg_mean = scale(np.concatenate([get_w2v_mean(z,200) for z in x_train]))\n",
    "train_vecs_cbow_sg_sum = scale(np.concatenate([get_w2v_sum(z,200) for z in x_train]))\n",
    "\n",
    "val_vecs_cbow_sg_mean = scale(np.concatenate([get_w2v_mean(z,200) for z in x_validation]))\n",
    "val_vecs_cbow_sg_sum = scale(np.concatenate([get_w2v_sum(z,200) for z in x_validation]))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_cbow_sg_mean, y_train)\n",
    "print('score',clf.score(val_vecs_cbow_sg_mean,y_validation))\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_cbow_sg_sum, y_train)\n",
    "print('score',clf.score(val_vecs_cbow_sg_sum,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embedding_index[w] = np.append(model_ug_cbow.wv[w], model_ug_sg.wv[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.4101411e-01,  7.0059037e-01,  8.3010459e-01, -7.6240128e-01,\n",
       "        9.2581913e-02, -3.9489813e-02, -3.3957180e-01, -1.9511002e-01,\n",
       "        1.3541147e+00, -1.7232928e-01, -3.2292948e+00, -6.1934370e-01,\n",
       "       -6.1544079e-01,  7.9256278e-01, -6.4990640e-01,  1.7944692e+00,\n",
       "        4.5546997e-01,  7.7703619e-01,  2.5435334e-01, -6.3907212e-01,\n",
       "        2.2822945e+00, -3.1438178e-01, -1.7934468e+00,  7.9437464e-01,\n",
       "       -1.0218585e+00,  1.7109734e+00,  7.7585500e-01,  1.5414184e-01,\n",
       "        7.4565250e-01,  1.5159445e+00,  1.2781268e+00,  1.4685680e+00,\n",
       "       -2.4625416e-01, -6.6322461e-03, -9.9509485e-02, -9.6901143e-01,\n",
       "        2.2899323e+00,  8.9628136e-01,  1.6564355e+00,  5.7904100e-01,\n",
       "        1.2763214e+00,  1.1964790e+00,  5.3964864e-02, -5.9881747e-01,\n",
       "       -3.3820705e+00, -3.0595270e-01,  4.3726271e-01,  6.4467233e-01,\n",
       "        1.5943125e+00,  1.0897419e+00,  8.7324619e-01, -5.0665039e-01,\n",
       "       -3.1724146e-01, -7.9429191e-01,  1.1440791e+00, -5.1362610e-01,\n",
       "        4.2860949e-01, -8.4514603e-02, -5.5597854e-01,  9.1720033e-01,\n",
       "        9.6830773e-01, -2.1726601e+00, -9.7309762e-01, -2.8062952e-01,\n",
       "        1.5055383e+00, -3.4888687e+00,  4.0586850e-01,  2.0224291e-01,\n",
       "        1.2235670e+00,  1.0700029e+00,  1.2727311e+00,  7.9717445e-01,\n",
       "       -1.2816864e+00, -3.3993116e-01,  2.1416096e-01,  1.1744064e-01,\n",
       "        1.8915623e+00,  5.6951320e-01, -1.9531503e+00, -6.3658684e-02,\n",
       "        1.5246847e+00, -2.1121073e+00,  2.5699232e+00, -1.5715461e+00,\n",
       "       -7.4303490e-01,  9.4940352e-01,  6.0775685e-01,  1.9038086e-01,\n",
       "       -1.3478507e-01,  1.6179098e+00,  7.2338927e-01, -1.2032217e-01,\n",
       "       -1.0315718e-01, -1.0558159e-01,  2.1789622e+00,  7.9378116e-01,\n",
       "        1.4734626e-02,  2.9680580e-01,  6.0469937e-01,  2.8366882e-01,\n",
       "        7.9567754e-04, -3.1393133e-03,  4.8407447e-03,  3.2344311e-03,\n",
       "       -1.2015369e-03, -1.9492470e-03, -4.7568758e-03,  1.5479974e-03,\n",
       "       -2.3419640e-03, -2.5984538e-03,  4.1360650e-03, -3.5758382e-03,\n",
       "       -1.1195866e-03,  4.1546915e-03, -2.0918788e-03, -1.8838607e-03,\n",
       "        3.1395230e-05,  3.9269105e-03, -4.0101158e-03,  1.7336040e-03,\n",
       "        4.9406858e-03, -8.5344835e-04, -4.7171959e-03,  4.6012816e-03,\n",
       "        1.0683801e-03, -4.0531150e-04,  2.7976716e-03, -2.0956439e-03,\n",
       "       -8.6124790e-05, -4.5971731e-03, -4.6068318e-03,  2.4893883e-04,\n",
       "        4.6511502e-03,  3.0953358e-03, -4.0281126e-03, -2.6351216e-04,\n",
       "       -4.9279644e-03,  1.2182487e-03,  1.6133590e-03,  1.9450964e-03,\n",
       "        2.5049113e-03,  2.6258184e-03, -2.7759892e-03,  4.8640801e-04,\n",
       "       -4.5694541e-03, -2.1120084e-03,  1.6950885e-03,  4.8608733e-03,\n",
       "       -3.6326962e-04, -2.7217199e-03,  2.0821118e-03,  1.3480783e-03,\n",
       "        3.3907800e-03,  1.0372003e-03, -1.2505963e-03, -2.9620570e-03,\n",
       "       -4.2576808e-03, -2.4020567e-03, -1.2939530e-03,  1.4513702e-03,\n",
       "       -3.8863267e-04, -2.5697697e-03,  3.0592151e-03, -3.9188275e-03,\n",
       "       -4.0920009e-03, -3.7159978e-03,  3.3122476e-05, -2.2092578e-03,\n",
       "       -3.1920893e-03,  2.9840673e-04,  1.1745355e-03,  4.3870318e-03,\n",
       "        1.7390207e-03, -4.6000346e-03, -3.8361193e-03,  3.1437757e-03,\n",
       "       -1.2588936e-03, -2.9394860e-04,  4.5618680e-03, -6.1586115e-04,\n",
       "       -1.5652013e-03,  1.8925639e-03,  3.5613347e-03, -1.6046407e-04,\n",
       "       -2.0866159e-03, -1.2331915e-03,  2.1377036e-04,  2.9048380e-03,\n",
       "        3.7098423e-04, -4.4117477e-03,  2.1748601e-03, -1.7424450e-03,\n",
       "        1.4786508e-03, -3.5217246e-03, -3.7825932e-03, -2.6717039e-03,\n",
       "        4.1489168e-03,  1.6118988e-04,  5.5512722e-04,  9.2332461e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index.get('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 2139900.47it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2780543.51it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2928316.83it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2566643.27it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2174118.88it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3118868.96it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2593004.73it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2675281.27it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2611644.90it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2890932.99it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2509609.95it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3083076.78it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2622014.25it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2581719.07it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2803394.91it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2453483.61it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2535478.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2996319.81it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2966286.33it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3025942.81it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2741871.93it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3124611.79it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2480586.65it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2815287.17it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2926910.28it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2821746.29it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2458658.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2880497.30it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2920788.37it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3036497.87it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2678220.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.928869420108469\n"
     ]
    }
   ],
   "source": [
    "# Distributed Memory Concatenation\n",
    "\n",
    "model_ug_dmc = Doc2Vec(dm=1,dm_concat=1,vector_size=100,negative=5,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_dmc.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v),epochs=1)\n",
    "    model_ug_dmc.alpha -=0.002\n",
    "    model_ug_dmc.min_alpha = model_ug_dmc.alpha\n",
    "\n",
    "train_vecs_dmc = get_vectors(model_ug_dmc, x_train, 100)\n",
    "validation_vecs_dmc = get_vectors(model_ug_dmc, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc, y_train)\n",
    "print('score',clf.score(validation_vecs_dmc,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31962/31962 [00:00<00:00, 2288895.91it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2653307.16it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2588198.79it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2621552.78it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2470438.49it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2599793.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2306221.41it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2737392.94it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2661102.18it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2832537.70it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2875862.80it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2487767.82it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2674427.33it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2752455.49it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2915199.07it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2453169.33it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2972402.93it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2853519.46it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2919643.36it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2621809.13it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2734154.81it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2975107.51it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3006264.31it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2716260.98it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2813809.89it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2586750.50it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2424113.86it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2650893.68it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2543898.15it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 3052052.28it/s]\n",
      "100%|██████████| 31962/31962 [00:00<00:00, 2914945.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9309553608677513\n"
     ]
    }
   ],
   "source": [
    "# Distributed Memory Mean\n",
    "\n",
    "model_ug_dmm = Doc2Vec(dm=1,dm_mean=1,vector_size=100,negative=5,min_count=2,workers=cores,alpha=0.1,min_alpha=0.1)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v),epochs=1)\n",
    "    model_ug_dmm.alpha -=0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha\n",
    "\n",
    "train_vecs_dmm = get_vectors(model_ug_dmm, x_train, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, x_validation , 100)\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, y_train)\n",
    "print('score',clf.score(validation_vecs_dmm,y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9413850646641635"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_train, 200)\n",
    "validation_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_validation, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc, y_train)\n",
    "clf.score(validation_vecs_dbow_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9397163120567376"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_train, 200)\n",
    "validation_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_validation, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm, y_train)\n",
    "clf.score(validation_vecs_dbow_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9309553608677513"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_dmm_dmc = get_concat_vectors(model_ug_dmm,model_ug_dmc, x_train, 200)\n",
    "validation_vecs_dmm_dmc = get_concat_vectors(model_ug_dmm,model_ug_dmc, x_validation, 200)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm_dmc, y_train)\n",
    "clf.score(validation_vecs_dmm_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
